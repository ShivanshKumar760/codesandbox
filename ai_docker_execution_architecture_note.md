# ğŸ³ AI Code Execution Architecture Using Docker (Loveable-Style Platform)

## ğŸ“Œ Overview

This document explains how AI-generated code is executed securely using Docker containers in a sandboxed environment. The architecture is designed for platforms similar to AI app builders where users provide prompts and receive working applications or script execution results.

The system consists of three major layers:

1. AI Code Generation Layer
2. Container Execution Layer (DockerManager)
3. Preview / Hosting Layer

---

# ğŸ§  1. High-Level Execution Flow

User Prompt â†“ AI generates code â†“ Backend receives generated code â†“ DockerManager.createContainer(userId) â†“ DockerManager.executeCode(userId, code) â†“ Code runs inside isolated Docker container â†“ Output streamed back to frontend



# ğŸ¯ Important Principle

Your DockerManager executes code.

The AI does NOT execute code.

The AI only:

- Generates code
- Modifies code
- Explains code
- Fixes errors

Execution is always done by your sandbox.



# ğŸ— Architecture for AI-Powered Code Platform

If you're building something like Loveable:

```
User
  â†“
Frontend (Editor UI)
  â†“
Backend API
  â†“
LLM Service (Generate Code)
  â†“
DockerManager.executeCode()
  â†“
Container
  â†“
  
Return Output
```

---

# ğŸ§  Step-by-Step Flow (Real Scenario)

### 1ï¸âƒ£ User says:

> "Create a simple express server"

### 2ï¸âƒ£ Backend calls LLM

```
const response = await openai.chat.completions.create({
  model: "gpt-4",
  messages: [{ role: "user", content: prompt }]
});

const generatedCode = response.choices[0].message.content;

```

Now you have code as string.



### 3ï¸âƒ£ Send Code to DockerManager

```
const result = await DockerManager.executeCode(userId, generatedCode);

```

This runs inside:

- 512MB container
- No network
- CPU limited
- Timeout 10 seconds





### 4ï¸âƒ£ Return Output

```
{
  "output": "...",
  "error": ""
}

```

---

# ğŸ”¥ So What Actually Happens Internally?

In your system:

### AI Layer

- Just text generator
- Lives separately
- Could be OpenAI API or your own model

### Execution Layer

- Your DockerManager
- Runs untrusted code

### Orchestrator Layer

- Connects AI â†’ Execution





# ğŸ— 2. Core Components in DockerManager

## ğŸ”¹ Imported Modules

- dockerode â†’ Communicates with Docker daemon
- uuid â†’ Generates unique container names
- pgPool â†’ Tracks container sessions in PostgreSQL
- tar-fs â†’ Builds Docker image from folder
- fs & path â†’ File system utilities

---

# ğŸ³ 3. Image Management

## Base Image

node:18-alpine

- Lightweight
- Secure minimal OS
- Fast startup

## Custom Sandbox Image

IMAGE\_NAME = "custom-node-sandbox\:latest"

The system:

1. Pulls the base image
2. Builds a sandbox image using a local Dockerfile
3. Tags it as custom-node-sandbox\:latest

This ensures:

- Controlled runtime
- Pre-installed dependencies (if needed)
- Isolated environment

---

# ğŸ“¦ 4. Ensuring Image Exists

Before creating containers:

- List existing images
- Check if sandbox image exists
- If not:
  - Pull base image
  - Build sandbox image

This prevents rebuilding on every request.

---

# ğŸš€ 5. Creating a Container

Each user gets a dedicated container.

Container configuration:

- Memory: 512MB
- MemorySwap: 512MB
- CpuQuota: Limited
- NetworkMode: none (no internet access)
- PidsLimit: 64
- WorkingDir: /workspace

Container runs:

"tail -f /dev/null"

This keeps container alive as a persistent sandbox.

The container ID is:

- Stored in memory (activeContainers map)
- Persisted in PostgreSQL (user\_sessions table)

---

# â–¶ 6. Executing AI-Generated Code

## Step 1: Write Code Safely

Code is converted to Base64:

- Prevents shell injection
- Prevents command breakouts

Then written inside container:

/workspace/index.js

---

## Step 2: Execute Code

Command executed inside container:

node /workspace/index.js

Execution results:

- stdout captured
- stderr captured
- Timeout enforced (e.g., 10 seconds)

Returned as:

{ output: string, error: string }

---

# â³ 7. Timeout Protection

runExec() implements:

- Stream monitoring
- Manual timeout
- Safe stream destruction

Prevents:

- Infinite loops
- Blocking execution
- Resource exhaustion

---

# ğŸ›‘ 8. Stopping and Cleanup

When user session ends:

- Stop container
- Remove container
- Delete DB session
- Remove from activeContainers map

Also supports:

- cleanupAll() for graceful shutdown
- SIGINT and SIGTERM handlers

---

# ğŸ›¡ 9. Security Mechanisms

1. Memory limits
2. CPU throttling
3. No network access
4. PID limits
5. Controlled working directory
6. Base64 write to avoid injection

This creates a secure sandbox runtime.

---

# ğŸŒ 10. Extending to Full App Builder (Loveable-Style)

Current system supports:

âœ” Script execution

To support full applications:

You would add:

- package.json support
- npm install inside container
- Dev server execution
- Port exposure
- Dynamic port binding
- Preview URL generation

Example upgrades:

- Expose container port 3000
- Bind to random host port
- Return preview link to frontend

---

# ğŸ§± 11. Scaling Considerations

Current limitations:

- In-memory container tracking
- Single server deployment

Production improvements:

- Store state fully in PostgreSQL
- Periodic reconciliation with Docker daemon
- Container health monitoring
- Background cleanup jobs
- Eventually migrate to Kubernetes



# ğŸ— Advanced AI Code Platform Architecture

```
                User
                  â†“
            Web IDE (Monaco)
                  â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Backend API       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“            â†“
        LLM Service   Workspace Manager
           â†“            â†“
        Code Gen     Docker Container
                          â†“
                    /workspace folder
                          â†“
                   node server.js
                          â†“
                   Stream logs back
```

---

# ğŸ§  Final Mental Model

AI generates code. Docker runs code in isolation. Database tracks sessions. Backend orchestrates execution. Frontend displays results.

This architecture forms the foundation of a cloud-based AI coding platform.

